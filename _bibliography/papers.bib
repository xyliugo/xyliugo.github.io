---
---


@article{wu2025synthrl,
  title={SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis},
  author={Wu*, Zijian and Ni*, Jinjie and Liu*, Xiangyan and Liu, Zichen and Yan, Hang and Shieh, Michael Qizhe},
  journal={arXiv preprint arXiv:2506.02096},
  url={https://arxiv.org/pdf/2506.02096},
  arxiv={2506.02096},
  code={https://github.com/NUS-TRAIL/SynthRL},
  year={2025},
  abbr={Preprint},
}

@article{wang2025fostering,
  title={Fostering Video Reasoning via Next-Event Prediction},
  author={Wang*, Haonan and Liu*, Hongfu and Liu, Xiangyan and Du, Chao and Kawaguchi, Kenji and Wang, Ye and Pang, Tianyu},
  journal={arXiv preprint arXiv:2505.22457},
  url={https://arxiv.org/pdf/2505.22457},
  arxiv={2505.22457},
  code={https://github.com/sail-sg/Video-Next-Event-Prediction},
  year={2025},
  abbr={Preprint},
}

@article{liu2025noisyrollout,
  title={NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation},
  author={Liu*, Xiangyan and Ni*, Jinjie and Wu*, Zijian and Du, Chao and Dou, Longxu and Wang, Haonan and Pang, Tianyu and Shieh, Michael Qizhe},
  journal={arXiv preprint arXiv:2504.13055},
  year={2025},
  url={https://arxiv.org/pdf/2504.13055},
  arxiv={2504.13055},
  code={https://github.com/NUS-TRAIL/NoisyRollout},
  selected={true},
  abbr={Preprint},
}

@inproceedings{liu2024codexgraph,
  title={CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases},
  author={Liu*, Xiangyan and Lan*, Bo and Hu, Zhiyuan and Liu, Yang and Zhang, Zhicheng and Wang, Fei and Shieh, Michael and Zhou, Wenmeng},
  booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics},
  year={2025},
  url={https://aclanthology.org/2025.naacl-long.7/},
  arxiv={2408.03910},
  code={https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent},
  selected={true},
  abbr={NAACL 2025},
}

@article{ji2024described,
  title={Described Spatial-Temporal Video Detection},
  author={Ji, Wei and Liu, Xiangyan and Sun, Yingfei and Deng, Jiajun and Qin, You and Nuwanna, Ammar and Qiu, Mengyao and Wei, Lina and Zimmermann, Roger},
  journal={arXiv preprint arXiv:2407.05610},
  url={https://arxiv.org/pdf/2407.05610},
  arxiv={2407.05610},
  year={2024},
  abbr={Preprint},
}

@inproceedings{liu2023towards,
  title={Towards Robust Multi-Modal Reasoning via Model Selection},
  author={Liu*, Xiangyan and Li*, Rongxue and Ji, Wei and Lin, Tao},
  journal={arXiv preprint arXiv:2310.08446},
  year={2024},
  selected={true},
  url={https://openreview.net/forum?id=KTf4DGAzus},
  arxiv={2310.08446},
  code={https://github.com/LINs-lab/M3},
  abbr={ICLR 2024},
}

@article{ji2024toward,
  title={Toward Complex-Query Referring Image Segmentation: A Novel Benchmark},
  author={Ji, Wei and Li, Li and Fei, Hao and Liu, Xiangyan and Yang, Xun and Li, Juncheng and Zimmermann, Roger},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  volume={21},
  number={1},
  pages={1--18},
  year={2024},
  url={https://arxiv.org/pdf/2309.17205},
  arxiv={2309.17205},
  publisher={ACM New York, NY},
  abbr={TOMM 2024},
}

@article{ni2023content,
  title={A Content-Driven Micro-Video Recommendation Dataset at Scale},
  author={Ni, Yongxin and Cheng, Yu and Liu, Xiangyan and Fu, Junchen and Li, Youhua and He, Xiangnan and Zhang, Yongfeng and Yuan, Fajie},
  journal={arXiv preprint arXiv:2309.15379},
  url={https://arxiv.org/pdf/2309.15379},
  arxiv={2309.15379},
  code={https://github.com/westlake-repl/MicroLens},
  year={2023},
  abbr={Preprint}
}

@inproceedings{ji2023online,
  title={Online Distillation-Enhanced Multi-modal Transformer for Sequential Recommendation},
  author={Ji*, Wei and Liu*, Xiangyan and Zhang, An and Wei, Yinwei and Ni, Yongxin and Wang, Xiang},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={955--965},
  url={https://arxiv.org/pdf/2308.04067},
  arxiv={2308.04067},
  code={https://github.com/xyliugo/ODMT},
  year={2023},
  abbr={MM 2023 (Oral)}
}